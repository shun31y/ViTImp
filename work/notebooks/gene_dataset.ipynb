{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b009a429-d36a-48d6-9d31-137e43482686",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "2326a006-2995-4caf-8d20-7df69255c011",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12116it [00:27, 437.59it/s]\n"
     ]
    }
   ],
   "source": [
    "def download_file(url, local_filename):\n",
    "    with requests.get(url, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        with open(local_filename, 'wb') as f:\n",
    "            for chunk in tqdm(r.iter_content(chunk_size=8192)): \n",
    "                f.write(chunk)\n",
    "url = \"https://www.phontron.com/kftt/download/kftt-data-1.0.tar.gz\"\n",
    "local_filename = \"../data/kftt-data-1.0.tar.gz\"\n",
    "#download_file(url, local_filename)\n",
    "#!tar -xvzf ../data/kftt-data-1.0.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "72d305ee-41cb-4dbe-9e6f-1ea6fcd9504d",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = '../data/kftt-data-1.0/data/tok/'\n",
    "\n",
    "train_file_ja_path = folder_path + 'kyoto-dev.ja'\n",
    "train_file_en_path = folder_path + 'kyoto-dev.en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "86899e2a-0831-4fae-ade7-d06acb8d3ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_ja(text):\n",
    "    return text.strip().split()\n",
    "\n",
    "def tokenizer_en(text):\n",
    "    return text.strip().lower().split()\n",
    "\n",
    "def text_iterator_ja(file_stream):\n",
    "    for line in file_stream:\n",
    "        yield line\n",
    "\n",
    "def text_iterator_en(file_stream):\n",
    "    for line in file_stream:\n",
    "        yield '<bos>'+' '+line+' '+'<eos>'\n",
    "\n",
    "def text_iterator_vocab(file_stream,tokenizer):\n",
    "    for line in file_stream:\n",
    "        yield tokenizer(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "ec21a445-0747-4583-8fd5-92831db7c8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "text_df = pd.DataFrame()\n",
    "with open(train_file_ja_path,\"r\") as f:\n",
    "  sentences_ja = list(text_iterator_ja(f))\n",
    "  sentences_ja = np.array(sentences_ja)\n",
    "with open(train_file_en_path,\"r\") as f:\n",
    "  sentences_en = list(text_iterator_en(f))\n",
    "  sentences_en = np.array(sentences_en)\n",
    "text_df['text_en'] = sentences_en\n",
    "text_df['text_ja'] = sentences_ja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "04b52bf9-3666-4656-843e-4eb1b2bfe1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "from torchtext.vocab import FastText\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a618841f-ee80-4b3e-9414-d050ac6e5a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(train_file_ja_path, \"r\") as f:\n",
    "    vocab_ja = torchtext.vocab.build_vocab_from_iterator(\n",
    "        iterator=text_iterator_vocab(f, tokenizer_ja),\n",
    "        specials = (['<pad>','<unk>','<eos>','<bos>'])\n",
    "    )\n",
    "\n",
    "with open(train_file_en_path, \"r\") as f:\n",
    "    vocab_en = torchtext.vocab.build_vocab_from_iterator(\n",
    "        iterator=text_iterator_vocab(f, tokenizer_en),\n",
    "        specials = (['<pad>','<unk>','<eos>','<bos>'])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "a3f1853c-7ec4-4ace-8f87-13252e172b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_ja.set_default_index(vocab_ja['<pad>'])\n",
    "vocab_en.set_default_index(vocab_en['<pad>'])\n",
    "\n",
    "vocab_ja.set_default_index(vocab_ja['<unk>'])\n",
    "vocab_en.set_default_index(vocab_en['<unk>'])\n",
    "\n",
    "vocab_ja.set_default_index(vocab_ja['<bos>'])\n",
    "vocab_en.set_default_index(vocab_en['<bos>'])\n",
    "\n",
    "vocab_ja.set_default_index(vocab_ja['<eos>'])\n",
    "vocab_en.set_default_index(vocab_en['<eos>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "7e46bbb9-19f6-49a1-a1d6-1ff6fece8bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_id_ja = [vocab_ja.lookup_indices(tokenizer_ja(text_list)) for text_list in sentences_ja]\n",
    "sentences_id_en = [vocab_en.lookup_indices(tokenizer_en(text_list)) for text_list in sentences_en]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "3261e3cc-0bdb-4332-8d6e-4d2c09766d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length_ja = np.max(np.array([len(item) for item in sentences_id_ja]))\n",
    "max_length_en = np.max(np.array([len(item) for item in sentences_id_en]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "8656e3dd-b424-43f4-9ee4-36c53bf2ff83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset,DataLoader\n",
    "class MyDataset(Dataset):\n",
    "  def __init__(self,data,vocab_en,vocab_ja):\n",
    "    self.data = data\n",
    "    self.sentences_ja = list(data['text_ja'])\n",
    "    self.sentences_en = list(data['text_en'])\n",
    "    self.vocab_en = vocab_en\n",
    "    self.vocab_ja = vocab_ja\n",
    "    self.vocab_size_ja = len(vocab_ja)\n",
    "    self.vocab_size_en = len(vocab_en)\n",
    "  def __len__(self):\n",
    "    return len(self.data)\n",
    "  def tokenizer_ja(self,text):\n",
    "    return text.strip().split()\n",
    "  def tokenizer_en(self,text):\n",
    "    return text.strip().lower().split()\n",
    "  def text_iterator_ja(self,file_stream):\n",
    "    for line in file_stream:\n",
    "        yield line\n",
    "  def text_iterator_en(self,file_stream):\n",
    "    for line in file_stream:\n",
    "        yield '<bos>'+' '+line+' '+'<eos>'\n",
    "  def text_iterator_vocab(self,file_stream,tokenizer):\n",
    "    for line in file_stream:\n",
    "        yield tokenizer(line)\n",
    "        \n",
    "  def __getitem__(self,idx):\n",
    "    tokenized_sentence_ja = self.text_iterator_vocab(self.sentences_ja,tokenizer_ja)\n",
    "    tokenized_sentence_en = self.text_iterator_vocab(self.sentences_en,tokenizer_en)\n",
    "    tokenized_sentence_ja = list(tokenized_sentence_ja)[idx]\n",
    "    tokenized_sentence_en = list(tokenized_sentence_en)[idx]\n",
    "    while(len(tokenized_sentence_ja) < MAX_LEN):\n",
    "      tokenized_sentence_ja.append('<pad>')\n",
    "    while(len(tokenized_sentence_en) < MAX_LEN):\n",
    "      tokenized_sentence_en.append('<pad>')\n",
    "    sentence_id_ja = self.vocab_ja.lookup_indices(tokenized_sentence_ja)\n",
    "    sentence_id_en = self.vocab_en.lookup_indices(tokenized_sentence_en)\n",
    "    sentence_id_ja = torch.tensor(sentence_id_ja,dtype=torch.long)\n",
    "    sentence_id_en = torch.tensor(sentence_id_en,dtype=torch.long)\n",
    "    return sentence_id_ja,sentence_id_en\n",
    "train_data,test_data = train_test_split(text_df,test_size=0.2)\n",
    "train_data = MyDataset(train_data,vocab_en,vocab_ja)\n",
    "test_data = MyDataset(test_data,vocab_en,vocab_ja)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "05b203c7-30fb-4656-8204-ab1df51aa415",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(vocab_ja, 'vocab_ja.pth')\n",
    "torch.save(vocab_en, 'vocab_en.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "59aa2360-94ed-4456-8152-52cfbcda2bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(train_data,'../data/train_data.pth')\n",
    "torch.save(test_data,'../data/test_data.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2e9bf620-14de-4a0a-a72c-4904d074641a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = np.max([max_length_ja,max_length_en])\n",
    "src_vocab_size = len(vocab_ja)\n",
    "tgt_vocab_size = len(vocab_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ac4fb8ef-64d7-4af7-ae10-ed255585dd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "config_data = {\n",
    "    'max_len':int(np.max([max_length_ja,max_length_en])),\n",
    "    'src_vocab_size':int(src_vocab_size),\n",
    "    'tgt_vocab_size':int(tgt_vocab_size),\n",
    "}\n",
    "\n",
    "with open('../data/config.json','w') as json_file:\n",
    "    json.dump(config_data, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "13c23b8d-fabe-4a8c-8f42-4ad7d65a18c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data,batch_size=16,shuffle=True,drop_last=True)\n",
    "test_loader = DataLoader(test_data,batch_size=16,shuffle=True,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "2d9aae55-8131-49a0-bcae-88ddf57b111a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文章の量は16\n",
      "en_textのshapeはtorch.Size([16, 159])\n",
      "最初のencoding文はtensor([   3,  101,    6, 2623,    8,  950,    2,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0])\n",
      "辞書の最初の30文字は['<pad>', '<unk>', '<eos>', '<bos>', 'the', ',', 'of', '.', 'and', 'in', '(', ')', 'to', 'was', 'a', '\"', 'is', 'as', \"'s\", 'that', 'by', 'kyoto', 'for', 'it', 'his', 'university', 'with', 'he', 'emperor', '-']\n",
      "文の最大長さは159\n",
      "------------------------------------------------\n",
      "文章の量は16\n",
      "ja_textのshapeはtorch.Size([16, 159])\n",
      "最初のencoding文はtensor([4458,  501,  190,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0])\n",
      "辞書の最初の30文字は['<pad>', '<unk>', '<eos>', '<bos>', 'の', '、', 'に', '。', 'は', 'を', 'る', 'た', 'て', 'と', 'し', '（', '）', 'が', 'い', '年', 'で', 'な', 'あ', 'っ', 'れ', '・', 'さ', 'り', '-', '京都']\n",
      "文の最大長さは159\n",
      "------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "tmp = iter(train_loader)\n",
    "ja_text = 0\n",
    "en_text = 0\n",
    "for batch in tmp:\n",
    "  ja_text,en_text = batch\n",
    "  print(\"文章の量は{}\".format(len(en_text)))\n",
    "  print(\"en_textのshapeは{}\".format(en_text.shape))\n",
    "  print(\"最初のencoding文は{}\".format(en_text[10][0:30]))\n",
    "  print(\"辞書の最初の30文字は{}\".format(vocab_en.lookup_tokens(range(30))))\n",
    "  print(\"文の最大長さは{}\".format(len(en_text[10])))\n",
    "  print(\"------------------------------------------------\")\n",
    "  print(\"文章の量は{}\".format(len(ja_text)))\n",
    "  print(\"ja_textのshapeは{}\".format(ja_text.shape))\n",
    "  print(\"最初のencoding文は{}\".format(ja_text[10][0:30]))\n",
    "  print(\"辞書の最初の30文字は{}\".format(vocab_ja.lookup_tokens(range(30))))\n",
    "  print(\"文の最大長さは{}\".format(len(ja_text[10])))\n",
    "  print(\"------------------------------------------------\")\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f301c298-e51f-4ad9-995c-5544a281213e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
