{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e378af7f-326e-4054-8a84-117d64f86bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd9ede31-2341-4a7d-9c9d-0eaff0deecf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_path = os.getcwd()\n",
    "parent_dir = os.path.dirname(notebook_path)\n",
    "sys.path.append(parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a4083f2-92ee-4fdc-b3f5-b539cf829316",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importnb\n",
    "with __import__('importnb').Notebook(): \n",
    "    from utils.TransformerDataset import MyDataset\n",
    "    from model import Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0443a833-8018-432e-b239-c591cbf788b8",
   "metadata": {},
   "source": [
    "## 各パラメータの指定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c72b52a8-92e8-44d3-9605-d4e5d8561bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('../data/config.json', 'r') as file:\n",
    "    config = json.load(file)\n",
    "    \n",
    "max_len = config[\"max_len\"]\n",
    "src_vocab_size = config[\"src_vocab_size\"]\n",
    "tgt_vocab_size = config[\"tgt_vocab_size\"]\n",
    "batch_size = 16\n",
    "num_head = 8\n",
    "d_model = 512\n",
    "d_ff =2048\n",
    "N = 6\n",
    "pad_idx = 0\n",
    "dropout_rate=0.1\n",
    "layer_norm_eps = 1e-5\n",
    "\n",
    "epoch = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10d47d0-5436-4be7-83c1-1db47011c533",
   "metadata": {},
   "source": [
    "## Datasetの読み込み、DataLoaderへの変形"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbb73f1f-3766-4584-adbc-eb03ac6bede5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torch.load('../data/train_data.pth')\n",
    "test_data = torch.load('../data/test_data.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5b3acc8-0f2c-4abd-956e-439b0ace3bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data,batch_size=16,shuffle=True,drop_last=True)\n",
    "test_loader = DataLoader(test_data,batch_size=16,shuffle=True,drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4298497d-32bc-4848-9b1b-afa948ae213b",
   "metadata": {},
   "source": [
    "## 辞書の読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e75546e-0e5f-4dc7-81bb-2dd4af82648f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "vocab_en = torch.load('../data/vocab_en.pth')\n",
    "vocab_ja = torch.load('../data/vocab_ja.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27aef7fe-16f1-4f56-b05a-01c69dc352de",
   "metadata": {},
   "source": [
    "### 一応データセット、辞書の確認　"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "778d9ab7-4db3-4e9c-889f-e4da066baee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文章の量は16\n",
      "en_textのshapeはtorch.Size([16, 159])\n",
      "最初のencoding文はtensor([   3,  935, 4190,    4,  842,  351,   33,  178,  410,  141,  271,  855,\n",
      "           4,  163,  180,    5,  935,    8,   58,   25, 1495,  280,    4,  881,\n",
      "           8,    4, 1489,  501, 1763,    7])\n",
      "辞書の最初の30文字は['<pad>', '<unk>', '<eos>', '<bos>', 'the', ',', 'of', '.', 'and', 'in', '(', ')', 'to', 'was', 'a', '\"', 'is', 'as', \"'s\", 'that', 'by', 'kyoto', 'for', 'it', 'his', 'university', 'with', 'he', 'emperor', '-']\n",
      "文の最大長さは159\n",
      "------------------------------------------------\n",
      "文章の量は16\n",
      "ja_textのshapeはtorch.Size([16, 159])\n",
      "最初のencoding文はtensor([1330,   31,   28,  275,  506,    6,  305,  660,  951,    9, 1875,   11,\n",
      "        3619,   13,  691,   20,   31,  305,  638,  106,   62, 1645,  207, 1324,\n",
      "           9,  837,    7,    0,    0,    0])\n",
      "辞書の最初の30文字は['<pad>', '<unk>', '<eos>', '<bos>', 'の', '、', 'に', '。', 'は', 'を', 'る', 'た', 'て', 'と', 'し', '（', '）', 'が', 'い', '年', 'で', 'な', 'あ', 'っ', 'れ', '・', 'さ', 'り', '-', '京都']\n",
      "文の最大長さは159\n",
      "------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "tmp = iter(train_loader)\n",
    "ja_text = 0\n",
    "en_text = 0\n",
    "for batch in tmp:\n",
    "  ja_text,en_text = batch\n",
    "  print(\"文章の量は{}\".format(len(en_text)))\n",
    "  print(\"en_textのshapeは{}\".format(en_text.shape))\n",
    "  print(\"最初のencoding文は{}\".format(en_text[0][0:30]))\n",
    "  print(\"辞書の最初の30文字は{}\".format(vocab_en.lookup_tokens(range(30))))\n",
    "  print(\"文の最大長さは{}\".format(len(en_text[0])))\n",
    "  print(\"------------------------------------------------\")\n",
    "  print(\"文章の量は{}\".format(len(ja_text)))\n",
    "  print(\"ja_textのshapeは{}\".format(ja_text.shape))\n",
    "  print(\"最初のencoding文は{}\".format(ja_text[0][0:30]))\n",
    "  print(\"辞書の最初の30文字は{}\".format(vocab_ja.lookup_tokens(range(30))))\n",
    "  print(\"文の最大長さは{}\".format(len(ja_text[0])))\n",
    "  print(\"------------------------------------------------\")\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78017dd3-26d1-4d2d-a14f-93558de304d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos> harvard university-as the 350th anniversary from their foundation came around almost the same year , harvard and ryukoku university jointly held the commemoration and the japan-u.s. international symposium . <eos>\n",
      "-------------------------------------------\n",
      "ハーバード 大学 - 同 時期 に 創立 350 周年 を 迎え た 本学 と 共同 で 大学 創立 記念 会 や 日米 国際 シンポジウム を 開催 。\n"
     ]
    }
   ],
   "source": [
    "words = [vocab_en.lookup_token(index) for index in en_text[0].tolist() if vocab_en.lookup_token(index) != '<pad>']\n",
    "sentence = ' '.join(words)\n",
    "print(sentence)\n",
    "print('-------------------------------------------')\n",
    "words = [vocab_ja.lookup_token(index) for index in ja_text[0].tolist() if vocab_ja.lookup_token(index) != '<pad>']\n",
    "sentence = ' '.join(words)\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ebd538d-6673-48a4-bb58-d3da58f618f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9b5bfe-b16d-4be8-b273-6885a0f9161d",
   "metadata": {},
   "source": [
    "## モデルの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e58b6411-90be-4779-b1cc-b98b5601844b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(\n",
    "    src_vocab_size=src_vocab_size,\n",
    "    tgt_vocab_size=tgt_vocab_size,\n",
    "    max_len=max_len,\n",
    "    num_head=num_head,\n",
    "    N=N,\n",
    "    pad_idx=pad_idx,\n",
    "    dropout_rate=dropout_rate,\n",
    "    layer_norm_eps=layer_norm_eps,\n",
    "    device = device\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc60f7a3-d2f8-4544-9f71-1b0dddd06d01",
   "metadata": {},
   "source": [
    "## 損失関数と最適化関数の定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d33f1b2c-164f-436f-a546-29dbae4d669e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                       lr=1e-3,\n",
    "                      betas=(0.9,0.98),\n",
    "                      eps=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19c5b502-0b90-4b39-aa94-814e516ba630",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WarmupLinearSchedule(torch.optim.lr_scheduler._LRScheduler):\n",
    "    def __init__(\n",
    "        self,\n",
    "        optimizer,\n",
    "        warmup_steps,\n",
    "        total_steps,\n",
    "        last_epoch=-1\n",
    "    )->None:\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.total_steps = total_steps\n",
    "        super(WarmupLinearSchedule, self).__init__(optimizer, last_epoch)\n",
    "    \n",
    "    def get_lr(\n",
    "        self\n",
    "    )->list:\n",
    "        step = self.last_epoch\n",
    "        if step < self.warmup_steps:\n",
    "            return [base_lr * float(step) / self.warmup_steps for base_lr in self.base_lrs]\n",
    "        return [base_lr * max(0.0, float(self.total_steps - step) / (self.total_steps - self.warmup_steps)) for base_lr in self.base_lrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d764faa-2804-4a4f-93f7-3e986591724f",
   "metadata": {},
   "outputs": [],
   "source": [
    "warmup_steps = 4000\n",
    "total_steps = epoch * ((len(train_loader.dataset)//batch_size)+(1 if len(train_loader.dataset) % batch_size > 0 else 0))\n",
    "scheduler = WarmupLinearSchedule(optimizer, warmup_steps,total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1035ba-ca1d-4411-a670-953e8e60f8f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------\n",
      "Epoch:1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 2/58 [00:32<15:11, 16.28s/it]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "train_loss_list = []\n",
    "test_loss_list = []\n",
    "\n",
    "for i in range(epoch):\n",
    "    print('---------------------')\n",
    "    print(\"Epoch:{}/{}\".format(i+1,epoch))\n",
    "    train_loss = 0\n",
    "    test_loss = 0\n",
    "\n",
    "    model.train()\n",
    "    for ja_text,en_text in tqdm(train_loader):\n",
    "        ja_text = ja_text.to(device)\n",
    "        en_text = en_text.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_pred_prob = model(\n",
    "            src=ja_text,\n",
    "            tgt=en_text\n",
    "        ).view(-1,tgt_vocab_size)\n",
    "        loss=criterion(\n",
    "            y_pred_prob,\n",
    "            en_text.view(-1)\n",
    "        )\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        train_loss_list.append(loss.item())\n",
    "    epoch_train_loss = train_loss/len(train_loader)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for ja_text,en_text in tqdm(test_loader):\n",
    "            ja_text = ja_text.to(device)\n",
    "            en_text = en_text.to(device)\n",
    "            y_pred_prob = model(\n",
    "                src=ja_text,\n",
    "                tgt=en_text\n",
    "            ).view(-1,tgt_vocab_size)\n",
    "            loss = criterion(y_pred_prob,\n",
    "                             en_text.view(-1)\n",
    "                            )\n",
    "\n",
    "            test_loss_list.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8b8e09-6f7d-4e17-8f80-bfd574fe0663",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522a1477-dceb-4626-9738-3d8439de0196",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(20, 6), sharey=True)\n",
    "sns.set(style=\"darkgrid\")\n",
    "palette = sns.color_palette(\"muted\")\n",
    "# グラフ1（リスト 'a' 用）\n",
    "sns.lineplot(ax=axes[0], x=np.arange(len(train_loss_list)), y=train_loss_list, palette=palette, marker='o', linewidth=2.5)\n",
    "axes[0].set_title(\"Train_loss/steps\", fontsize=16, fontweight='bold')\n",
    "axes[0].set_xlabel(\"Step\", fontsize=14)\n",
    "axes[0].set_ylabel(\"Loss\", fontsize=14)\n",
    "\n",
    "sns.lineplot(ax=axes[1], x=np.arange(len(test_loss_list)), y=test_loss_list, palette=palette, marker='o', linewidth=2.5)\n",
    "axes[1].set_title(\"Test_loss/steps\", fontsize=16, fontweight='bold')\n",
    "axes[1].set_xlabel(\"Step\", fontsize=14)\n",
    "axes[1].set_ylabel(\"Loss\", fontsize=14)\n",
    "\n",
    "# グラフの表示\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
